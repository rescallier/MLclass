{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:31:00.935088Z",
     "start_time": "2018-10-09T14:31:00.852980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part9/8-828msg3.txt\n",
      "email is a spam: False\n",
      "Subject: english ` a ' versus ` some '\n",
      "\n",
      "could someone direct me to literature concerned with the semantics of english ` a ' contrasted with english ` some ' ? of course , details of relevant literature dealing with any language other than english would also be most welcome . i would appreciate it if you could e-mail me directly , i will post a summary if appropriate . many thanks in advance , - shravan vasishth university address : dept . of linguistics , osu , 1712 neil ave . , columbus , oh 43210 , ( usa ) office phone : ( 1-614 ) - 292-3802 home [ and preferred mailing ] address : 549 harley drive # 10 , columbus , oh 43202 , ( usa ) home phone / fax : ( 1-614 ) - 268-8217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 0 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:33:10.652989Z",
     "start_time": "2018-10-09T14:33:06.022590Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:33:20.250716Z",
     "start_time": "2018-10-09T14:33:20.188615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/robin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:36:28.608133Z",
     "start_time": "2018-10-09T14:36:22.480670Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:36:28.629323Z",
     "start_time": "2018-10-09T14:36:28.609702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:37:12.633846Z",
     "start_time": "2018-10-09T14:37:12.618441Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: english ` a ' versus ` some '\n",
      "\n",
      "could someone direct me to literature concerned with the semantics of english ` a ' contrasted with english ` some ' ? of course , details of relevant literature dealing with any language other than english would also be most welcome . i would appreciate it if you could e-mail me directly , i will post a summary if appropriate . many thanks in advance , - shravan vasishth university address : dept . of linguistics , osu , 1712 neil ave . , columbus , oh 43210 , ( usa ) office phone : ( 1-614 ) - 292-3802 home [ and preferred mailing ] address : 549 harley drive # 10 , columbus , oh 43202 , ( usa ) home phone / fax : ( 1-614 ) - 268-8217\n",
      "\n",
      "Bag of words representation (34 words in dict):\n",
      "{'drive': 1, 'preferred': 1, 'home': 2, 'phone': 2, 'office': 1, 'oh': 2, 'ave': 1, 'linguistics': 1, 'address': 2, 'university': 1, 'advance': 1, 'thanks': 1, 'many': 1, 'appropriate': 1, 'summary': 1, 'post': 1, 'directly': 1, 'mail': 1, 'appreciate': 1, 'welcome': 1, 'also': 1, 'would': 2, 'language': 1, 'dealing': 1, 'relevant': 1, 'course': 1, 'semantics': 1, 'concerned': 1, 'literature': 2, 'direct': 1, 'someone': 1, 'could': 2, 'versus': 1, 'subject': 1}\n",
      "\n",
      "Vector reprensentation (34 non-zero elements):\n",
      "  (0, 3839)\t1\n",
      "  (0, 9615)\t1\n",
      "  (0, 5886)\t2\n",
      "  (0, 9193)\t2\n",
      "  (0, 8566)\t1\n",
      "  (0, 8576)\t2\n",
      "  (0, 974)\t1\n",
      "  (0, 7261)\t1\n",
      "  (0, 173)\t2\n",
      "  (0, 13427)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 12711)\t1\n",
      "  (0, 7561)\t1\n",
      "  (0, 679)\t1\n",
      "  (0, 12266)\t1\n",
      "  (0, 9499)\t1\n",
      "  (0, 3525)\t1\n",
      "  (0, 7486)\t1\n",
      "  (0, 671)\t1\n",
      "  (0, 13998)\t1\n",
      "  (0, 428)\t1\n",
      "  (0, 14188)\t2\n",
      "  (0, 7019)\t1\n",
      "  (0, 3115)\t1\n",
      "  (0, 10436)\t1\n",
      "  (0, 2880)\t1\n",
      "  (0, 11207)\t1\n",
      "  (0, 2499)\t1\n",
      "  (0, 7296)\t2\n",
      "  (0, 3521)\t1\n",
      "  (0, 11704)\t1\n",
      "  (0, 2857)\t2\n",
      "  (0, 13732)\t1\n",
      "  (0, 12153)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:40:42.964414Z",
     "start_time": "2018-10-09T14:40:42.930818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:40:49.415428Z",
     "start_time": "2018-10-09T14:40:49.408262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 12153)\t0.03896841956452861\n",
      "  (0, 13732)\t0.202994088228446\n",
      "  (0, 2857)\t0.22421065791619868\n",
      "  (0, 11704)\t0.1494281641091214\n",
      "  (0, 3521)\t0.14613309633516322\n",
      "  (0, 7296)\t0.2885640109082825\n",
      "  (0, 2499)\t0.17643901426115144\n",
      "  (0, 11207)\t0.12062534677056168\n",
      "  (0, 2880)\t0.12943051418106705\n",
      "  (0, 10436)\t0.1400960081361585\n",
      "  (0, 3115)\t0.18236968877568285\n",
      "  (0, 7019)\t0.06646705880628265\n",
      "  (0, 14188)\t0.16948839703778934\n",
      "  (0, 428)\t0.07705536507140107\n",
      "  (0, 13998)\t0.13833243028709208\n",
      "  (0, 671)\t0.19001197566052927\n",
      "  (0, 7486)\t0.07652227304278053\n",
      "  (0, 3525)\t0.13991601511608456\n",
      "  (0, 9499)\t0.13631637282065065\n",
      "  (0, 12266)\t0.13054397334684056\n",
      "  (0, 679)\t0.15302780640455532\n",
      "  (0, 7561)\t0.09875550446793106\n",
      "  (0, 12711)\t0.1241755860100349\n",
      "  (0, 230)\t0.16141035689857305\n",
      "  (0, 13427)\t0.06544434821387468\n",
      "  (0, 173)\t0.16666335882919764\n",
      "  (0, 7261)\t0.07294963229171182\n",
      "  (0, 974)\t0.18811070007255262\n",
      "  (0, 8576)\t0.4177006963399086\n",
      "  (0, 8566)\t0.14175448904733468\n",
      "  (0, 9193)\t0.215876138790284\n",
      "  (0, 5886)\t0.24692545837638363\n",
      "  (0, 9615)\t0.16465960641967545\n",
      "  (0, 3839)\t0.1808002785765761\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:43:16.717961Z",
     "start_time": "2018-10-09T14:43:13.235910Z"
    }
   },
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:44:08.409209Z",
     "start_time": "2018-10-09T14:44:08.399755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part9/9-463msg1.txt\n",
      "email is a spam: False\n",
      "Subject: tsd98 workshop - - 2nd call for papers\n",
      "\n",
      "second announcement and call for papers a workshop on text , speech and dialog ( tsd ' 98 ) brno , czech republic , 23-26 september 1998 the workshop is organized by the faculty of informatics , masaryk university , brno , and the faculty of applied sciences , university of west bohemia , pilsen , under the auspices of the dean of the faculty of informatics of masaryk university . please visit the workshop 's homepage : http : / / www . fi . muni . cz / tsd98 / venue brno , czech republic topics tsd ' 98 will be concerned with topics in the field of natural language processing , in particular : - corpora , texts and transcription - speech analysis , recognition and synthesis - their intertwining within nl dialog systems . topics of the workshop will include ( but are not limited to ) : - text corpora and tagging - transcription problems in spoken corpora - sense disambiguation - links between text and speech oriented systems - parsing issues , especially parsing problems in spoken texts - multilingual issues , especially multilingual dialog systems - information retrieval and text / topic summarization - speech modeling - speech segmentation - speech recognition - text - to-speech synthesis - dialog systems - development of dialog strategies - assistive technologies based on speech and dialog - applied systems and software program committee baudoin genevieve ( france ) ferencz attila ( romania ) hanks patrick ( great britain , chair ) hermansky hynek ( usa ) kopecek ivan ( czech republic ) krishnamurthy ramesh ( great britain ) matousek vaclav ( czech republic ) mueller johannes ( germany ) noeth elmar ( germany ) pala karel ( czech republic ) pavesic nikola ( slovenia ) rubio antonio ( spain ) schukat - talamazzini e . guenter ( germany ) skrelin pavel ( russia ) organizing committee bartek ludek batusek robert komarkova dana ( secretary ) e-mail : dkomar @ fi . muni . cz kopecek ivan ( chair ) e-mail : kopecek @ fi . muni . cz matousek vaclav pala karel smrz pavel staudek jan zackova eva ( principal contact ) e-mail : glum @ fi . muni . cz zizka jan submission of papers abstracts of no more than 500 words [ plain ascii text only , please ] should be submitted to the following e-mail address on or before may 15 , 1998 : glum @ fi . muni . cz submissions should include , in addition to the abstract itself , the name of the author ( s ) , affiliation , address , telephone number , fax number , and e-mail address . electronic submissions will be acknowledged by e-mail , so please contact us if no acknowledgement is received . acceptance of submissions will likewise be notified by e-mail . accepted papers will be published in the proceedings of tsd ' 98 . authors of abstracts that are accepted will be requested to send their papers in postscript form ( in llncs format ) to the above by e-mail before august 17th . latex word processor is preferred but not required . format instructions ( and llncs latex format ) will be sent to authors together with the notification of acceptance . requests for participation will be processed on a \" first come first served \" basis . important dates friday , may 15 , 1998 . . . . . submissions of abstracts due tuesday , june 30 , 1998 . . . . . notification of acceptance sent to the authors monday , august 17 , 1998 . . . . . final papers ( camera ready ) due wednesday , september 23 , 1998 . . . . . . workshop date fees and costs registration fee : 80 . - usd ( includes proceedings , refreshments , social events and trip ) accommodation and food : double room ( shared with other participant ) : 130 . - usd single room : 190 . - usd the full cost of the workshop will therefore be either 210 , - usd or 270 , - usd , depending on whether accommodation is shared . further details will be announced later . official language the official language of the event will be english , but papers on issues relating to text and speech processing in languages other than english are strongly encouraged . address all correspondence regarding the workshop should be addressed to : dana komarkova faculty of informatics masaryk university botanicka 68a 60200 brno czech republic tel . : + 420 5 41 512 359 fax : e-mail : dkomar @ fi . muni . cz outline of the programme all sessions of the workshop will be plenary ( no parallel sessions ) . the format will consist of paper presentations ( generally 20 minutes ) followed by discussion ( 10 minutes ) . the workshop will also include social events , an excursion to the faculty of informatics , masaryk university brno , and a trip in the vicinity of brno ( the moravian karst , including the beautiful macocha chasm ) . location hotel myslivna , where the workshop will take place , is a comfortable hotel in beautiful woods on a hill near a natural reservation area very close to brno . the surrounding is very quiet and suitable for walks and hiking ( jogging ) routes . brno is the capital of moravia , which is in the south-east part of the czech republic . it is the second-largest town in the czech republic ( with a population of about half a million ) . it had been a royal city since 1347 . there are six universities in brno . historical and artistic places of interest include : - - brno castle ( now called spilberk ) - - veveri castle - - the old and new city halls - - the augustine monastery , with st thomas ' church and crypt of moravian margraves - - the church of st james - - the \" bishops ' church \" of st peter & st paul - - the famous villa tugendhadt , designed by mies van der rohe - - many other important examples of czech architecture between the wars ( 1918-38 ) . in the immediate surroundings of brno are the moravian karst . with macocha chasm and punkva caves ; the site of the \" battle of the three emperors \" ( napoleon , alexander of russia , and franz of austria ) , commonly known as the battle of austerlitz ; the chateau of slavkov ( austerlitz ) ; pernstejn castle ; and many other attractions . how to reach brno brno can be reached easily by direct trains from prague , vienna , bratislava , and budapest , or by plane to vienna and then by coach or train ( 130 km ) . another possibility is to go by plane to prague and then travel about 200 km by coach or train . further travel details will be given in future announcements . ivan kopecek kopecek @ fi . muni . cz http : / / www . fi . muni . cz / ~ kopecek /\n",
      "\n",
      "Bag of words representation (201 words in dictionary):\n",
      "{'possibility': 1, 'coach': 2, 'plane': 2, 'reach': 1, 'chateau': 1, 'known': 1, 'commonly': 1, 'napoleon': 1, 'battle': 2, 'surroundings': 1, 'immediate': 1, 'architecture': 1, 'villa': 1, 'famous': 1, 'peter': 1, 'crypt': 1, 'church': 3, 'monastery': 1, 'castle': 3, 'artistic': 1, 'historical': 1, 'six': 1, 'royal': 1, 'million': 1, 'half': 1, 'population': 1, 'town': 1, 'east': 1, 'capital': 1, 'suitable': 1, 'quiet': 1, 'surrounding': 1, 'close': 1, 'reservation': 1, 'near': 1, 'hill': 1, 'comfortable': 1, 'hotel': 2, 'location': 1, 'chasm': 2, 'beautiful': 2, 'karst': 2, 'vicinity': 1, 'excursion': 1, 'discussion': 1, 'generally': 1, 'paper': 1, 'consist': 1, 'parallel': 1, 'session': 2, 'outline': 1, 'regarding': 1, 'correspondence': 1, 'strongly': 1, 'event': 1, 'official': 2, 'later': 1, 'depending': 1, 'therefore': 1, 'single': 1, 'participant': 1, 'room': 2, 'double': 1, 'trip': 2, 'date': 1, 'ready': 1, 'camera': 1, 'due': 2, 'important': 2, 'participation': 1, 'notification': 2, 'together': 1, 'processor': 1, 'latex': 2, 'format': 4, 'postscript': 1, 'accepted': 2, 'notified': 1, 'likewise': 1, 'acceptance': 3, 'acknowledged': 1, 'telephone': 1, 'affiliation': 1, 'name': 1, 'addition': 1, 'following': 1, 'ascii': 1, 'plain': 1, 'glum': 2, 'russia': 2, 'johannes': 1, 'chair': 2, 'program': 1, 'based': 1, 'assistive': 1, 'segmentation': 1, 'modeling': 1, 'summarization': 1, 'topic': 1, 'retrieval': 1, 'multilingual': 2, 'especially': 2, 'sense': 1, 'within': 1, 'intertwining': 1, 'synthesis': 2, 'recognition': 2, 'transcription': 2, 'particular': 1, 'venue': 1, 'fi': 8, 'visit': 1, 'dean': 1, 'auspex': 1, 'west': 1, 'applied': 2, 'faculty': 5, 'organized': 1, 'text': 7, 'announcement': 1, 'designed': 1, 'interest': 1, 'city': 2, 'another': 1, 'full': 1, 'whether': 1, 'call': 2, 'site': 1, 'part': 1, 'area': 1, 'limited': 1, 'speech': 9, 'principal': 1, 'field': 1, 'u': 1, 'author': 1, 'development': 1, 'either': 1, 'received': 1, 'since': 1, 'natural': 2, 'corpus': 3, 'old': 1, 'word': 1, 'van': 1, 'republic': 8, 'social': 2, 'final': 1, 'spoken': 2, 'analysis': 1, 'three': 1, 'plenary': 1, 'link': 1, 'cost': 1, 'go': 1, 'place': 1, 'take': 1, 'new': 1, 'second': 2, 'august': 2, 'st': 4, 'june': 1, 'submission': 1, 'abstract': 1, 'include': 4, 'send': 1, 'information': 1, 'electronic': 1, 'committee': 2, 'future': 1, 'secretary': 1, 'fee': 1, 'number': 2, 'may': 2, 'workshop': 11, 'sent': 2, 'south': 1, 'train': 2, 'easily': 1, 'travel': 2, 'contact': 2, 'form': 1, 'please': 3, 'food': 1, 'registration': 1, 'basis': 1, 'come': 1, 'first': 2, 'given': 1, 'accommodation': 2, 'great': 2, 'preferred': 1, 'address': 4, 'university': 5, 'many': 2, 'mail': 9, 'also': 1, 'language': 3, 'concerned': 1, 'direct': 1, 'subject': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
